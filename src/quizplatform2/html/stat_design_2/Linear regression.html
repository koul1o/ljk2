<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Linear regression</title>
        <link href="css/bootstrap.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
    </head>
    <body >
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.0/jquery.js"></script>
        <script src="js/bootstrap.min.js"></script>
        <script src="js/quizplatform.js"></script>



        <div id="logos" style="overflow: auto;"> 
            <img src="img/logo_gipsa.jpg" style="width: 200px; height: 150px; margin: auto; float: left;"> </img>
            <img src="img/logo_ljk.jpg" style="width: 200px; height: 150px; margin: auto; float: right; "> </img>
        </div>  

        <div id="main">
            <div style="background-color: #e2e2e2;">
                <h1 align="center">Linear regression</h1>  
            </div>

            <div id="left">
                <div class="panel-group" id="accordion">
                    <div class="panel panel-default" id="Panel 1">
                        <div class="panel-heading">
                            <h4 class="panel-title">
                                <a data-toggle="collapse" onclick="sendElementTrace();" data-parent="#accordion" href="#collapseOne">
                                    Introduction
                                </a>
                            </h4>
                        </div>
                        <div id="collapseOne" class="panel-collapse collapse in">
                            <div class="panel-body">
						<img src="img/linear_regression.png" align="right">
						In statistics, linear regression is an approach for modeling the relationship between a scalar <a href="Dependent and independent variables.html">dependent variable</a> <i>Y</i> and one or more explanatory variables (or <a href="Dependent and independent variables.html">independent variables</a>) denoted <i>X<sub>1</sub></i>, <i>X<sub>2</sub></i>... <i>X<sub>p</sub></i>. The relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Most commonly, the conditional mean of <i>Y</i> given the value of the other variables is assumed to be an affine function of the variables.</br></br>
						The goal can be the prediction of the dependent variable based on values of the explanatory variables (for instance to predict the probable values of future observations), or the estimation of the explanatory power of each variable on <i>Y</i> on a given data sample (to determine which variables have a strong influence on the dependent variable, or on which variables efforts should be focused).</br></br>
						The linearity of the model means that the relationship between the variables is represented as a straight line (for instance the red line going through the cloud of observation points on the figure). Although linearity is the simplest model one can consider when modeling the complexity of the world (see <a href="Degrees of freedom.html">degrees of freedom</a>), it is often sufficient to detect general trends in patterns.
                            </div>
                       </div>
                    </div>
                    <div class="panel panel-default" id="Panel 2">
                        <div class="panel-heading">
                            <h4 class="panel-title">
                                <a data-toggle="collapse" onclick="sendElementTrace();" data-parent="#accordion" href="#collapseTwo">
                                    Technical details
                                </a>
                            </h4>
                        </div>
                        <div id="collapseTwo" class="panel-collapse collapse">
                            <div class="panel-body">
						<!-- <img src="img/linear_regression.svg" style="vertical-align: -1.005ex; width:44.959ex; height:2.843ex;"/> -->
						The basic model for linear regression is : <i>Y<sub>i</sub></i> = <i>b<sub>0</sub></i> + <i>b<sub>1</sub> X<sub>i1</sub></i> + <i>b<sub>2</sub> X<sub>i2</sub></i> + ... + <i>b<sub>p</sub> X<sub>ip</sub></i> + <i>e<sub>i</sub></i></br></br>
						Where we consider <i>n</i> observations of one dependent variable (<i>Y</i>) and <i>p</i> independent variables (<i>X<sub>1</sub></i>... <i>X<sub>p</sub></i>). Thus, <i>Y<sub>i</sub></i> is the i<sup>th</sup> observation of the dependent variable, <i>X<sub>ij</sub></i> is i<sup>th</sup> observation of the j<sup>th</sup> independent variable, <i>j</i> = 1, 2, ..., <i>p</i>. The values <i>b<sub>j</sub></i> represent the estimates of the parameters on the sample, and <i>e<sub>i</sub></i> is the error for the i<sup>th</sup> observation.</br></br>
						The random errors <i>e<sub>i</sub></i> are also called residuals, as they correspond to the difference between the predictions of the model, and the observed values of the dependent variable. They are what remains once everything that is explained by the model is removed. Errors are assumed to be independent, identically distributed, and normal.</br></br>
						Although many methods exist to estimate the parameter values, they usually rely on least squares estimation (for instance ordinary least squares - OLS), which aims at maximizing the explained variance, and thus minimizing the residuals around the regression line. This is sometimes called the BLUE, standing for the Best Linear Unbiased Estimator.</br></br>
						To determine if a model was useful for explaining the data, one can compare this model to the one corresponding to the null hypothesis (i.e. no effect of the independent variables) using a Fisher test, which is one way to perforrm <a href="Statistical hypothesis testing.html">statistical hypothesis testing</a>. This Fisher test (or F-test) checks how much the residuals are reduced with the full model relatively to the null hypothesis (<i>F</i> = explained variance / unexplained variance). Yet, statistical significance is only obtained if the difference is sufficient, relatively to the number of observations (<i>n</i> in the above equations) and to the number of parameters of the model (<i>p</i>) (please refer to <a href="Degrees of freedom.html">degrees of freedom</a> for more details).</br></br>
						After the model has been fitted to the data, it also possible to test if individual parameter estimates <i>b<sub>j</sub></i> are statistically different from 0. This means that the associated variable <i>X<sub>j</sub></i> has a significant effect on <i>Y</i>. For this purpose, the same method is used, again relying on <a href="Statistical hypothesis testing.html">statistical hypothesis testing</a>, but this time performing a Student's t-test. This test provides positive results if the estimated distribution of the parameter (due to uncertainty on the estimated mean value) is sufficiently distant from 0</br></br>
                            </div>
                        </div>
                    </div>
                    <div class="panel panel-default" id="Panel 3">
                        <div class="panel-heading">
                            <h4 class="panel-title">
                                <a data-toggle="collapse" onclick="sendElementTrace();" data-parent="#accordion" href="#collapseThree" >
                                    Avoid confusion and errors
                                </a>
                            </h4>
                        </div>
                        <div id="collapseThree" class="panel-collapse collapse">
                            <div class="panel-body">
						<img src="img/Anscombe_quartet.png" width="400px" align="right">						
						Linear regression is a special case of the general linear model, where there can be several dependent variables. Additionally, based on the number of independent variables, the names simple linear regression and multiple linear regressions are often used. These terms are distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.</br></br>
						Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of <i>Y</i> given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis.</br></br>
						As for most statistical method, there are conditions to meet in order to apply linear regression correctly, and avoid incorrect inferences. In the case of linear regression, residuals must satisfy the three main following constraints:
						<ul>
							<li>Normality: The distribution of the errors should follow a Gaussian profile, else the line might not be representative of the data. This may for instance happen if outliers pull the line away from the rest of the observations, or if a linear model is not a good model of the data structure.</li>
							<li>Homoscedasticity: The dispersion (variance) of the residuals should not depend on the explanatory variables. The worst case being a funnel type distribution of residuals, biasing the results and leading to higher statistical errors.</li>
							<li>Independence: There should be no structure in the residuals (for instance detemrined by other measured variables). It else means some dependence between the observations should have been taken into account in the model (for instance repeated measures).</li>
						</ul>
						These three conditions are summarized by stating that <i>e<sub>i</sub></i> are independent identically distributed normal errors. The figure on the right illustrates different cases of non normality and heteroscedasticity (violation of homoscedasticity).</br></br>
						An additional condition to verify, in order to avoid missing significant effects of specific explanatory variables, is the absence of multicollineraity. Multicollinearity corresponds to the presence of correlated predictor variables, which then share the explained variance of the dependent variable. They therefore loose their individual potential of prediction of the dependent variable.
                            </div>
                        </div>
                    </div>
                    <div id="buttons_area">
                    <a href="#" id="back" class="btn btn-info"  role="button" onclick="backToQuiz();" style="float: right;">Back to the Quiz</a>

                </div>

                <div id="message_completed"> </div>
            </div>
            </div> <!-- end left -->

            <div id="right">
                <div id="documents"> 
                    <h2>Documents</h2>  

                </div>

            </div>


        </div>


    </body>
</html>